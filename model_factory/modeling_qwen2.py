# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# #           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.
# #               Do NOT edit this file manually as any edits will be overwritten by the generation of
# #             the file from the modular. If any change should be done, please apply the change to the
# #                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.
# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# # coding=utf-8
# # Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
# #
# # This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# # and OPT implementations in this library. It has been modified from its
# # original forms to accommodate minor architectural differences compared
# # to GPT-NeoX and OPT used by the Meta AI team that trained the model.
# #
# # Licensed under the Apache License, Version 2.0 (the "License");
# # you may not use this file except in compliance with the License.
# # You may obtain a copy of the License at
# #
# #     http://www.apache.org/licenses/LICENSE-2.0
# #
# # Unless required by applicable law or agreed to in writing, software
# # distributed under the License is distributed on an "AS IS" BASIS,
# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# # See the License for the specific language governing permissions and
# # limitations under the License.
# from transformers.configuration_utils import PretrainedConfig, layer_type_validation
# from transformers.modeling_rope_utils import rope_config_validation


# class Qwen2_5_VLVisionConfig(PretrainedConfig):
#     model_type = "qwen2_5_vl"
#     base_config_key = "vision_config"

#     def __init__(
#         self,
#         depth=32,
#         hidden_size=3584,
#         hidden_act="silu",
#         intermediate_size=3420,
#         num_heads=16,
#         in_channels=3,
#         patch_size=14,
#         spatial_merge_size=2,
#         temporal_patch_size=2,
#         tokens_per_second=4,
#         window_size=112,
#         out_hidden_size=3584,
#         fullatt_block_indexes=[7, 15, 23, 31],
#         initializer_range=0.02,
#         **kwargs,
#     ):
#         super().__init__(**kwargs)

#         self.depth = depth
#         self.hidden_size = hidden_size
#         self.hidden_act = hidden_act
#         self.intermediate_size = intermediate_size
#         self.num_heads = num_heads
#         self.in_channels = in_channels
#         self.patch_size = patch_size
#         self.spatial_merge_size = spatial_merge_size
#         self.temporal_patch_size = temporal_patch_size
#         self.tokens_per_second = tokens_per_second
#         self.window_size = window_size
#         self.fullatt_block_indexes = fullatt_block_indexes
#         self.out_hidden_size = out_hidden_size
#         self.initializer_range = initializer_range


# class Qwen2_5_VLTextConfig(PretrainedConfig):
#     r"""
#     This is the configuration class to store the configuration of a [`Qwen2_5_VLTextModel`]. It is used to instantiate a
#     Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration
#     with the defaults will yield a similar configuration to that of
#     Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).

#     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
#     documentation from [`PretrainedConfig`] for more information.

#     Args:
#         vocab_size (`int`, *optional*, defaults to 152064):
#             Vocabulary size of the Qwen2_5_VL model. Defines the number of different tokens that can be represented by the
#             `inputs_ids` passed when calling [`Qwen2_5_VLModel`]
#         hidden_size (`int`, *optional*, defaults to 8192):
#             Dimension of the hidden representations.
#         intermediate_size (`int`, *optional*, defaults to 29568):
#             Dimension of the MLP representations.
#         num_hidden_layers (`int`, *optional*, defaults to 80):
#             Number of hidden layers in the Transformer encoder.
#         num_attention_heads (`int`, *optional*, defaults to 64):
#             Number of attention heads for each attention layer in the Transformer encoder.
#         num_key_value_heads (`int`, *optional*, defaults to 8):
#             This is the number of key_value heads that should be used to implement Grouped Query Attention. If
#             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
#             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
#             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
#             by meanpooling all the original heads within that group. For more details, check out [this
#             paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.
#         hidden_act (`str` or `function`, *optional*, defaults to `"silu"`):
#             The non-linear activation function (function or string) in the decoder.
#         max_position_embeddings (`int`, *optional*, defaults to 32768):
#             The maximum sequence length that this model might ever be used with.
#         initializer_range (`float`, *optional*, defaults to 0.02):
#             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
#         rms_norm_eps (`float`, *optional*, defaults to 1e-05):
#             The epsilon used by the rms normalization layers.
#         use_cache (`bool`, *optional*, defaults to `True`):
#             Whether or not the model should return the last key/values attentions (not used by all models). Only
#             relevant if `config.is_decoder=True`.
#         tie_word_embeddings (`bool`, *optional*, defaults to `False`):
#             Whether the model's input and output word embeddings should be tied.
#         rope_theta (`float`, *optional*, defaults to 1000000.0):
#             The base period of the RoPE embeddings.
#         use_sliding_window (`bool`, *optional*, defaults to `False`):
#             Whether to use sliding window attention.
#         sliding_window (`int`, *optional*, defaults to 4096):
#             Sliding window attention (SWA) window size. If not specified, will default to `4096`.
#         max_window_layers (`int`, *optional*, defaults to 80):
#             The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any
#             additional layer afterwards will use SWA (Sliding Window Attention).
#         layer_types (`list`, *optional*):
#             Attention pattern for each layer.
#         attention_dropout (`float`, *optional*, defaults to 0.0):
#             The dropout ratio for the attention probabilities.
#         rope_scaling (`Dict`, *optional*):
#             Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
#             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value
#             accordingly.
#             Expected contents:
#                 `rope_type` (`str`):
#                     The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',
#                     'llama3'], with 'default' being the original RoPE implementation.
#                 `factor` (`float`, *optional*):
#                     Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In
#                     most scaling types, a `factor` of x will enable the model to handle sequences of length x *
#                     original maximum pre-trained length.
#                 `original_max_position_embeddings` (`int`, *optional*):
#                     Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during
#                     pretraining.
#                 `attention_factor` (`float`, *optional*):
#                     Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention
#                     computation. If unspecified, it defaults to value recommended by the implementation, using the
#                     `factor` field to infer the suggested value.
#                 `beta_fast` (`float`, *optional*):
#                     Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear
#                     ramp function. If unspecified, it defaults to 32.
#                 `beta_slow` (`float`, *optional*):
#                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear
#                     ramp function. If unspecified, it defaults to 1.
#                 `short_factor` (`list[float]`, *optional*):
#                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<
#                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden
#                     size divided by the number of attention heads divided by 2
#                 `long_factor` (`list[float]`, *optional*):
#                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<
#                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden
#                     size divided by the number of attention heads divided by 2
#                 `low_freq_factor` (`float`, *optional*):
#                     Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE
#                 `high_freq_factor` (`float`, *optional*):
#                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE
#         image_token_id (`int`, *optional*):
#             Token index used as placeholder for image embeddings.
#         video_token_id (`int`, *optional*):
#             Token index used as placeholder for video embeddings.

#     ```python
#     >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig

#     >>> # Initializing a Qwen2_5_VL style configuration
#     >>> configuration = Qwen2_5_VLConfig()

#     >>> # Initializing a model from the Qwen2-VL-7B style configuration
#     >>> model = Qwen2_5_VLTextModel(configuration)

#     >>> # Accessing the model configuration
#     >>> configuration = model.config
#     ```"""

#     model_type = "qwen2_5_vl_text"
#     base_config_key = "text_config"
#     keys_to_ignore_at_inference = ["past_key_values"]
#     # Default tensor parallel plan for base model `Qwen2_5_VL`
#     base_model_tp_plan = {
#         "layers.*.self_attn.q_proj": "colwise",
#         "layers.*.self_attn.k_proj": "colwise",
#         "layers.*.self_attn.v_proj": "colwise",
#         "layers.*.self_attn.o_proj": "rowwise",
#         "layers.*.mlp.gate_proj": "colwise",
#         "layers.*.mlp.up_proj": "colwise",
#         "layers.*.mlp.down_proj": "rowwise",
#     }
#     base_model_pp_plan = {
#         "embed_tokens": (["input_ids"], ["inputs_embeds"]),
#         "layers": (["hidden_states", "attention_mask"], ["hidden_states"]),
#         "norm": (["hidden_states"], ["hidden_states"]),
#     }

#     def __init__(
#         self,
#         vocab_size=152064,
#         hidden_size=8192,
#         intermediate_size=29568,
#         num_hidden_layers=80,
#         num_attention_heads=64,
#         num_key_value_heads=8,
#         hidden_act="silu",
#         max_position_embeddings=32768,
#         initializer_range=0.02,
#         rms_norm_eps=1e-05,
#         use_cache=True,
#         tie_word_embeddings=False,
#         rope_theta=1000000.0,
#         use_sliding_window=False,
#         sliding_window=4096,
#         max_window_layers=80,
#         layer_types=None,
#         attention_dropout=0.0,
#         rope_scaling=None,
#         image_token_id=None,
#         video_token_id=None,
#         **kwargs,
#     ):
#         self.vocab_size = vocab_size
#         self.max_position_embeddings = max_position_embeddings
#         self.hidden_size = hidden_size
#         self.intermediate_size = intermediate_size
#         self.num_hidden_layers = num_hidden_layers
#         self.num_attention_heads = num_attention_heads
#         self.use_sliding_window = use_sliding_window
#         self.sliding_window = sliding_window if self.use_sliding_window else None
#         self.max_window_layers = max_window_layers

#         # for backward compatibility
#         if num_key_value_heads is None:
#             num_key_value_heads = num_attention_heads

#         self.num_key_value_heads = num_key_value_heads
#         self.hidden_act = hidden_act
#         self.initializer_range = initializer_range
#         self.rms_norm_eps = rms_norm_eps
#         self.use_cache = use_cache
#         self.rope_theta = rope_theta
#         self.attention_dropout = attention_dropout
#         self.rope_scaling = rope_scaling

#         self.layer_types = layer_types
#         if self.layer_types is None:
#             self.layer_types = [
#                 "sliding_attention"
#                 if self.sliding_window is not None and i >= self.max_window_layers
#                 else "full_attention"
#                 for i in range(self.num_hidden_layers)
#             ]
#         layer_type_validation(self.layer_types, self.num_hidden_layers)

#         # Validate the correctness of rotary position embeddings parameters
#         # BC: if there is a 'type' field, move it to 'rope_type'.
#         # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations
#         # one can set it to "linear"/"dynamic" etc. to have scaled RoPE
#         # TODO: @raushan update config in the hub
#         if self.rope_scaling is not None and "type" in self.rope_scaling:
#             if self.rope_scaling["type"] == "mrope":
#                 self.rope_scaling["type"] = "default"
#             self.rope_scaling["rope_type"] = self.rope_scaling["type"]
#         rope_config_validation(self, ignore_keys={"mrope_section"})
#         self.image_token_id = image_token_id
#         self.video_token_id = video_token_id

#         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)


# class Qwen2_5_VLConfig(PretrainedConfig):
#     r"""
#     This is the configuration class to store the configuration of a [`Qwen2_5_VLModel`]. It is used to instantiate a
#     Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration
#     with the defaults will yield a similar configuration to that of
#     Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).

#     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
#     documentation from [`PretrainedConfig`] for more information.


#     Args:
#         text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2_5_VLTextConfig`):
#             The config object or dictionary of the text backbone.
#         vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2_5_VLVisionConfig`):
#             The config object or dictionary of the vision backbone.
#         image_token_id (`int`, *optional*, defaults to 151655):
#             The image token index to encode the image prompt.
#         video_token_id (`int`, *optional*, defaults to 151656):
#             The video token index to encode the image prompt.

#     ```python
#     >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig

#     >>> # Initializing a Qwen2_5_VL style configuration
#     >>> configuration = Qwen2_5_VLConfig()

#     >>> # Initializing a model from the Qwen2-VL-7B style configuration
#     >>> model = Qwen2_5_VLForConditionalGeneration(configuration)

#     >>> # Accessing the model configuration
#     >>> configuration = model.config
#     ```"""

#     model_type = "qwen2_5_vl"
#     sub_configs = {"vision_config": Qwen2_5_VLVisionConfig, "text_config": Qwen2_5_VLTextConfig}
#     keys_to_ignore_at_inference = ["past_key_values"]

#     def __init__(
#         self,
#         text_config=None,
#         vision_config=None,
#         image_token_id=151655,
#         video_token_id=151656,
#         **kwargs,
#     ):
#         if isinstance(vision_config, dict):
#             self.vision_config = self.sub_configs["vision_config"](**vision_config)
#         elif vision_config is None:
#             self.vision_config = self.sub_configs["vision_config"]()

#         if isinstance(text_config, dict):
#             self.text_config = self.sub_configs["text_config"](**text_config)
#         elif text_config is None:
#             # For BC use all kwargs to init `TextConfig`
#             self.text_config = self.sub_configs["text_config"](**kwargs)

#         self.image_token_id = image_token_id
#         self.video_token_id = video_token_id

#         super().__init__(**kwargs)


# __all__ = ["Qwen2_5_VLConfig", "Qwen2_5_VLTextConfig"]

# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# #           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.
# #               Do NOT edit this file manually as any edits will be overwritten by the generation of
# #             the file from the modular. If any change should be done, please apply the change to the
# #                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.
# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# # coding=utf-8
# # Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
# #
# # This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# # and OPT implementations in this library. It has been modified from its
# # original forms to accommodate minor architectural differences compared
# # to GPT-NeoX and OPT used by the Meta AI team that trained the model.
# #
# # Licensed under the Apache License, Version 2.0 (the "License");
# # you may not use this file except in compliance with the License.
# # You may obtain a copy of the License at
# #
# #     http://www.apache.org/licenses/LICENSE-2.0
# #
# # Unless required by applicable law or agreed to in writing, software
# # distributed under the License is distributed on an "AS IS" BASIS,
# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# # See the License for the specific language governing permissions and
# # limitations under the License.

# from dataclasses import dataclass
# from typing import Any, Callable, Optional, Union

# import torch
# import torch.nn as nn
# import torch.nn.functional as F

# from transformers.activations import ACT2FN
# from transformers.cache_utils import Cache, DynamicCache
# from transformers.generation import GenerationMixin
# from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
# from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
# from transformers.modeling_layers import GradientCheckpointingLayer
# from transformers.modeling_outputs import BaseModelOutputWithPast, ModelOutput
# from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
# from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
# from transformers.processing_utils import Unpack
# from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging
# from transformers.utils.deprecation import deprecate_kwarg
# from transformers.models.qwen2.modeling_qwen2 import Qwen2RMSNorm
# # from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig


# logger = logging.get_logger(__name__)


# class Qwen2_5_VLMLP(nn.Module):
#     def __init__(self, config, bias: bool = False):
#         super().__init__()
#         self.hidden_size = config.hidden_size
#         self.intermediate_size = config.intermediate_size
#         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)
#         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)
#         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=bias)
#         self.act_fn = ACT2FN[config.hidden_act]

#     def forward(self, hidden_state):
#         return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))


# class Qwen2_5_VisionPatchEmbed(nn.Module):
#     def __init__(
#         self,
#         patch_size: int = 14,
#         temporal_patch_size: int = 2,
#         in_channels: int = 3,
#         embed_dim: int = 1152,
#     ) -> None:
#         super().__init__()
#         self.patch_size = patch_size
#         self.temporal_patch_size = temporal_patch_size
#         self.in_channels = in_channels
#         self.embed_dim = embed_dim

#         kernel_size = [temporal_patch_size, patch_size, patch_size]
#         self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=False)

#     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
#         target_dtype = self.proj.weight.dtype
#         hidden_states = hidden_states.view(
#             -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
#         )
#         hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
#         return hidden_states


# class Qwen2_5_VisionRotaryEmbedding(nn.Module):
#     inv_freq: torch.Tensor  # fix linting for `register_buffer`

#     def __init__(self, dim: int, theta: float = 10000.0) -> None:
#         super().__init__()
#         inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
#         self.register_buffer("inv_freq", inv_freq, persistent=False)

#     def forward(self, seqlen: int) -> torch.Tensor:
#         seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
#         freqs = torch.outer(seq, self.inv_freq)
#         return freqs


# class Qwen2_5_VLPatchMerger(nn.Module):
#     def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> None:
#         super().__init__()
#         self.hidden_size = context_dim * (spatial_merge_size**2)
#         self.ln_q = Qwen2RMSNorm(context_dim, eps=1e-6)
#         self.mlp = nn.Sequential(
#             nn.Linear(self.hidden_size, self.hidden_size),
#             nn.GELU(),
#             nn.Linear(self.hidden_size, dim),
#         )

#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))
#         return x


# def rotate_half(x):
#     """Rotates half the hidden dims of the input."""
#     x1 = x[..., : x.shape[-1] // 2]
#     x2 = x[..., x.shape[-1] // 2 :]
#     return torch.cat((-x2, x1), dim=-1)


# def apply_rotary_pos_emb_vision(
#     q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
# ) -> tuple[torch.Tensor, torch.Tensor]:
#     orig_q_dtype = q.dtype
#     orig_k_dtype = k.dtype
#     q, k = q.float(), k.float()
#     cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
#     q_embed = (q * cos) + (rotate_half(q) * sin)
#     k_embed = (k * cos) + (rotate_half(k) * sin)
#     q_embed = q_embed.to(orig_q_dtype)
#     k_embed = k_embed.to(orig_k_dtype)
#     return q_embed, k_embed


# def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
#     """
#     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
#     num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
#     """
#     batch, num_key_value_heads, slen, head_dim = hidden_states.shape
#     if n_rep == 1:
#         return hidden_states
#     hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
#     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


# def eager_attention_forward(
#     module: nn.Module,
#     query: torch.Tensor,
#     key: torch.Tensor,
#     value: torch.Tensor,
#     attention_mask: Optional[torch.Tensor],
#     scaling: float,
#     dropout: float = 0.0,
#     **kwargs,
# ):
#     key_states = repeat_kv(key, module.num_key_value_groups)
#     value_states = repeat_kv(value, module.num_key_value_groups)

#     attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
#     if attention_mask is not None:
#         causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
#         attn_weights = attn_weights + causal_mask

#     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
#     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
#     attn_output = torch.matmul(attn_weights, value_states)
#     attn_output = attn_output.transpose(1, 2).contiguous()

#     return attn_output, attn_weights


# class Qwen2_5_VLVisionAttention(nn.Module):
#     def __init__(self, config: Qwen2_5_VLVisionConfig) -> None:
#         super().__init__()
#         self.dim = config.hidden_size
#         self.num_heads = config.num_heads
#         self.head_dim = self.dim // self.num_heads
#         self.num_key_value_groups = 1  # needed for eager attention
#         self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)
#         self.proj = nn.Linear(self.dim, self.dim)
#         self.scaling = self.head_dim**-0.5
#         self.config = config
#         self.attention_dropout = 0.0
#         self.is_causal = False

#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         cu_seqlens: torch.Tensor,
#         rotary_pos_emb: Optional[torch.Tensor] = None,
#         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
#         **kwargs,
#     ) -> torch.Tensor:
#         seq_length = hidden_states.shape[0]
#         query_states, key_states, value_states = (
#             self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
#         )
#         cos, sin = position_embeddings
#         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)

#         query_states = query_states.transpose(0, 1).unsqueeze(0)
#         key_states = key_states.transpose(0, 1).unsqueeze(0)
#         value_states = value_states.transpose(0, 1).unsqueeze(0)

#         attention_interface: Callable = eager_attention_forward
#         if self.config._attn_implementation != "eager":
#             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

#         if self.config._attn_implementation == "flash_attention_2":
#             # Flash Attention 2: Use cu_seqlens for variable length attention
#             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
#             attn_output, _ = attention_interface(
#                 self,
#                 query_states,
#                 key_states,
#                 value_states,
#                 attention_mask=None,
#                 scaling=self.scaling,
#                 dropout=0.0 if not self.training else self.attention_dropout,
#                 cu_seq_lens_q=cu_seqlens,
#                 cu_seq_lens_k=cu_seqlens,
#                 max_length_q=max_seqlen,
#                 max_length_k=max_seqlen,
#                 is_causal=False,
#                 **kwargs,
#             )
#         else:
#             # Other implementations: Process each chunk separately
#             lengths = cu_seqlens[1:] - cu_seqlens[:-1]
#             splits = [
#                 torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
#             ]

#             attn_outputs = [
#                 attention_interface(
#                     self,
#                     q,
#                     k,
#                     v,
#                     attention_mask=None,
#                     scaling=self.scaling,
#                     dropout=0.0 if not self.training else self.attention_dropout,
#                     is_causal=False,
#                     **kwargs,
#                 )[0]
#                 for q, k, v in zip(*splits)
#             ]
#             attn_output = torch.cat(attn_outputs, dim=1)

#         attn_output = attn_output.reshape(seq_length, -1).contiguous()
#         attn_output = self.proj(attn_output)
#         return attn_output


# class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):
#     def __init__(self, config, attn_implementation: str = "sdpa") -> None:
#         super().__init__()
#         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)
#         self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)
#         self.attn = Qwen2_5_VLVisionAttention(config=config)
#         self.mlp = Qwen2_5_VLMLP(config, bias=True)

#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         cu_seqlens: torch.Tensor,
#         rotary_pos_emb: Optional[torch.Tensor] = None,
#         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
#         **kwargs,
#     ) -> torch.Tensor:
#         hidden_states = hidden_states + self.attn(
#             self.norm1(hidden_states),
#             cu_seqlens=cu_seqlens,
#             rotary_pos_emb=rotary_pos_emb,
#             position_embeddings=position_embeddings,
#             **kwargs,
#         )
#         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
#         return hidden_states


# @auto_docstring
# class Qwen2_5_VLPreTrainedModel(PreTrainedModel):
#     config: Qwen2_5_VLConfig
#     base_model_prefix = "model"
#     supports_gradient_checkpointing = True
#     _no_split_modules = ["Qwen2_5_VLDecoderLayer", "Qwen2_5_VLVisionBlock"]
#     _skip_keys_device_placement = "past_key_values"
#     _supports_flash_attn = True
#     _supports_sdpa = True

#     _can_compile_fullgraph = True
#     _supports_attention_backend = True


# class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):
#     config: Qwen2_5_VLVisionConfig
#     _no_split_modules = ["Qwen2_5_VLVisionBlock"]

#     def __init__(self, config, *inputs, **kwargs) -> None:
#         super().__init__(config, *inputs, **kwargs)
#         self.spatial_merge_size = config.spatial_merge_size
#         self.patch_size = config.patch_size
#         self.fullatt_block_indexes = config.fullatt_block_indexes
#         self.window_size = config.window_size
#         self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size

#         self.patch_embed = Qwen2_5_VisionPatchEmbed(
#             patch_size=config.patch_size,
#             temporal_patch_size=config.temporal_patch_size,
#             in_channels=config.in_channels,
#             embed_dim=config.hidden_size,
#         )

#         head_dim = config.hidden_size // config.num_heads
#         self.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding(head_dim // 2)

#         self.blocks = nn.ModuleList([Qwen2_5_VLVisionBlock(config) for _ in range(config.depth)])
#         self.merger = Qwen2_5_VLPatchMerger(
#             dim=config.out_hidden_size,
#             context_dim=config.hidden_size,
#             spatial_merge_size=config.spatial_merge_size,
#         )
#         self.gradient_checkpointing = False

#     def rot_pos_emb(self, grid_thw):
#         pos_ids = []
#         for t, h, w in grid_thw:
#             hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
#             hpos_ids = hpos_ids.reshape(
#                 h // self.spatial_merge_size,
#                 self.spatial_merge_size,
#                 w // self.spatial_merge_size,
#                 self.spatial_merge_size,
#             )
#             hpos_ids = hpos_ids.permute(0, 2, 1, 3)
#             hpos_ids = hpos_ids.flatten()

#             wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
#             wpos_ids = wpos_ids.reshape(
#                 h // self.spatial_merge_size,
#                 self.spatial_merge_size,
#                 w // self.spatial_merge_size,
#                 self.spatial_merge_size,
#             )
#             wpos_ids = wpos_ids.permute(0, 2, 1, 3)
#             wpos_ids = wpos_ids.flatten()
#             pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
#         pos_ids = torch.cat(pos_ids, dim=0)
#         max_grid_size = grid_thw[:, 1:].max()
#         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
#         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
#         return rotary_pos_emb

#     def get_window_index(self, grid_thw):
#         window_index: list = []
#         cu_window_seqlens: list = [0]
#         window_index_id = 0
#         vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size

#         for grid_t, grid_h, grid_w in grid_thw:
#             llm_grid_h, llm_grid_w = (
#                 grid_h // self.spatial_merge_size,
#                 grid_w // self.spatial_merge_size,
#             )
#             index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(grid_t, llm_grid_h, llm_grid_w)
#             pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size
#             pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size
#             num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size
#             num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size
#             index_padded = F.pad(index, (0, pad_w, 0, pad_h), "constant", -100)
#             index_padded = index_padded.reshape(
#                 grid_t,
#                 num_windows_h,
#                 vit_merger_window_size,
#                 num_windows_w,
#                 vit_merger_window_size,
#             )
#             index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(
#                 grid_t,
#                 num_windows_h * num_windows_w,
#                 vit_merger_window_size,
#                 vit_merger_window_size,
#             )
#             seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)
#             index_padded = index_padded.reshape(-1)
#             index_new = index_padded[index_padded != -100]
#             window_index.append(index_new + window_index_id)
#             cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1]
#             cu_window_seqlens.extend(cu_seqlens_tmp.tolist())
#             window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()
#         window_index = torch.cat(window_index, dim=0)

#         return window_index, cu_window_seqlens

#     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:
#         """
#         Args:
#             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
#                 The final hidden states of the model.
#             grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
#                 The temporal, height and width of feature shape of each image in LLM.

#         Returns:
#             `torch.Tensor`: hidden_states.
#         """
#         hidden_states = self.patch_embed(hidden_states)
#         rotary_pos_emb = self.rot_pos_emb(grid_thw)
#         window_index, cu_window_seqlens = self.get_window_index(grid_thw)
#         cu_window_seqlens = torch.tensor(
#             cu_window_seqlens,
#             device=hidden_states.device,
#             dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
#         )
#         cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)

#         seq_len, _ = hidden_states.size()
#         hidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
#         hidden_states = hidden_states[window_index, :, :]
#         hidden_states = hidden_states.reshape(seq_len, -1)
#         rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
#         rotary_pos_emb = rotary_pos_emb[window_index, :, :]
#         rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
#         emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
#         position_embeddings = (emb.cos(), emb.sin())

#         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
#             dim=0,
#             # Select dtype based on the following factors:
#             #  - FA2 requires that cu_seqlens_q must have dtype int32
#             #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw
#             # See https://github.com/huggingface/transformers/pull/34852 for more information
#             dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
#         )
#         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)

#         for layer_num, blk in enumerate(self.blocks):
#             if layer_num in self.fullatt_block_indexes:
#                 cu_seqlens_now = cu_seqlens
#             else:
#                 cu_seqlens_now = cu_window_seqlens

#             hidden_states = blk(
#                 hidden_states,
#                 cu_seqlens=cu_seqlens_now,
#                 position_embeddings=position_embeddings,
#                 **kwargs,
#             )

#         hidden_states = self.merger(hidden_states)
#         reverse_indices = torch.argsort(window_index)
#         hidden_states = hidden_states[reverse_indices, :]

#         return hidden_states

# coding=utf-8
# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Qwen2VL model configuration"""

from transformers.configuration_utils import PretrainedConfig, layer_type_validation
from transformers.modeling_rope_utils import rope_config_validation
from transformers.utils import logging


logger = logging.get_logger(__name__)


class Qwen2VLVisionConfig(PretrainedConfig):
    model_type = "qwen2_vl"
    base_config_key = "vision_config"

    def __init__(
        self,
        depth=32,
        embed_dim=1280,
        hidden_size=3584,
        hidden_act="quick_gelu",
        mlp_ratio=4,
        num_heads=16,
        in_channels=3,
        patch_size=14,
        spatial_merge_size=2,
        temporal_patch_size=2,
        initializer_range=0.02,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.depth = depth
        self.embed_dim = embed_dim
        self.hidden_size = hidden_size
        self.hidden_act = hidden_act
        self.mlp_ratio = mlp_ratio
        self.num_heads = num_heads
        self.in_channels = in_channels
        self.patch_size = patch_size
        self.spatial_merge_size = spatial_merge_size
        self.temporal_patch_size = temporal_patch_size
        self.initializer_range = initializer_range


class Qwen2VLTextConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`Qwen2VLTextModel`]. It is used to instantiate a
    Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of
    Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PretrainedConfig`] for more information.

    Args:
        vocab_size (`int`, *optional*, defaults to 152064):
            Vocabulary size of the Qwen2VL model. Defines the number of different tokens that can be represented by the
            `inputs_ids` passed when calling [`Qwen2VLModel`]
        hidden_size (`int`, *optional*, defaults to 8192):
            Dimension of the hidden representations.
        intermediate_size (`int`, *optional*, defaults to 29568):
            Dimension of the MLP representations.
        num_hidden_layers (`int`, *optional*, defaults to 80):
            Number of hidden layers in the Transformer encoder.
        num_attention_heads (`int`, *optional*, defaults to 64):
            Number of attention heads for each attention layer in the Transformer encoder.
        num_key_value_heads (`int`, *optional*, defaults to 8):
            This is the number of key_value heads that should be used to implement Grouped Query Attention. If
            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
            by meanpooling all the original heads within that group. For more details, check out [this
            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.
        hidden_act (`str` or `function`, *optional*, defaults to `"silu"`):
            The non-linear activation function (function or string) in the decoder.
        max_position_embeddings (`int`, *optional*, defaults to 32768):
            The maximum sequence length that this model might ever be used with.
        initializer_range (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
        rms_norm_eps (`float`, *optional*, defaults to 1e-05):
            The epsilon used by the rms normalization layers.
        use_cache (`bool`, *optional*, defaults to `True`):
            Whether or not the model should return the last key/values attentions (not used by all models). Only
            relevant if `config.is_decoder=True`.
        tie_word_embeddings (`bool`, *optional*, defaults to `False`):
            Whether the model's input and output word embeddings should be tied.
        rope_theta (`float`, *optional*, defaults to 1000000.0):
            The base period of the RoPE embeddings.
        use_sliding_window (`bool`, *optional*, defaults to `False`):
            Whether to use sliding window attention.
        sliding_window (`int`, *optional*, defaults to 4096):
            Sliding window attention (SWA) window size. If not specified, will default to `4096`.
        max_window_layers (`int`, *optional*, defaults to 80):
            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any
            additional layer afterwards will use SWA (Sliding Window Attention).
        layer_types (`list`, *optional*):
            Attention pattern for each layer.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            The dropout ratio for the attention probabilities.
        rope_scaling (`Dict`, *optional*):
            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value
            accordingly.
            Expected contents:
                `rope_type` (`str`):
                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',
                    'llama3'], with 'default' being the original RoPE implementation.
                `factor` (`float`, *optional*):
                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In
                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *
                    original maximum pre-trained length.
                `original_max_position_embeddings` (`int`, *optional*):
                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during
                    pretraining.
                `attention_factor` (`float`, *optional*):
                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention
                    computation. If unspecified, it defaults to value recommended by the implementation, using the
                    `factor` field to infer the suggested value.
                `beta_fast` (`float`, *optional*):
                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear
                    ramp function. If unspecified, it defaults to 32.
                `beta_slow` (`float`, *optional*):
                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear
                    ramp function. If unspecified, it defaults to 1.
                `short_factor` (`list[float]`, *optional*):
                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<
                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden
                    size divided by the number of attention heads divided by 2
                `long_factor` (`list[float]`, *optional*):
                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<
                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden
                    size divided by the number of attention heads divided by 2
                `low_freq_factor` (`float`, *optional*):
                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE
                `high_freq_factor` (`float`, *optional*):
                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE
        image_token_id (`int`, *optional*):
            Token index used as placeholder for image embeddings.
        video_token_id (`int`, *optional*):
            Token index used as placeholder for video embeddings.

    ```python
    >>> from transformers import Qwen2VLTextModel, Qwen2VLConfig

    >>> # Initializing a Qwen2VL style configuration
    >>> configuration = Qwen2VLConfig()

    >>> # Initializing a model from the Qwen2-VL-7B style configuration
    >>> model = Qwen2VLTextModel(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "qwen2_vl_text"
    base_config_key = "text_config"
    keys_to_ignore_at_inference = ["past_key_values"]
    # Default tensor parallel plan for base model `Qwen2VL`
    base_model_tp_plan = {
        "layers.*.self_attn.q_proj": "colwise",
        "layers.*.self_attn.k_proj": "colwise",
        "layers.*.self_attn.v_proj": "colwise",
        "layers.*.self_attn.o_proj": "rowwise",
        "layers.*.mlp.gate_proj": "colwise",
        "layers.*.mlp.up_proj": "colwise",
        "layers.*.mlp.down_proj": "rowwise",
    }
    base_model_pp_plan = {
        "embed_tokens": (["input_ids"], ["inputs_embeds"]),
        "layers": (["hidden_states", "attention_mask"], ["hidden_states"]),
        "norm": (["hidden_states"], ["hidden_states"]),
    }

    def __init__(
        self,
        vocab_size=152064,
        hidden_size=8192,
        intermediate_size=29568,
        num_hidden_layers=80,
        num_attention_heads=64,
        num_key_value_heads=8,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-05,
        use_cache=True,
        tie_word_embeddings=False,
        rope_theta=1000000.0,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=80,
        layer_types=None,
        attention_dropout=0.0,
        rope_scaling=None,
        image_token_id=None,
        video_token_id=None,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.use_sliding_window = use_sliding_window
        self.sliding_window = sliding_window if self.use_sliding_window else None
        self.max_window_layers = max_window_layers

        # for backward compatibility
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads

        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.attention_dropout = attention_dropout
        self.rope_scaling = rope_scaling

        self.layer_types = layer_types
        if self.layer_types is None:
            self.layer_types = [
                "sliding_attention"
                if self.sliding_window is not None and i >= self.max_window_layers
                else "full_attention"
                for i in range(self.num_hidden_layers)
            ]
        layer_type_validation(self.layer_types, self.num_hidden_layers)

        # Validate the correctness of rotary position embeddings parameters
        # BC: if there is a 'type' field, move it to 'rope_type'.
        # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations
        # one can set it to "linear"/"dynamic" etc. to have scaled RoPE
        # TODO: @raushan update config in the hub
        if self.rope_scaling is not None and "type" in self.rope_scaling:
            if self.rope_scaling["type"] == "mrope":
                self.rope_scaling["type"] = "default"
            self.rope_scaling["rope_type"] = self.rope_scaling["type"]
        rope_config_validation(self, ignore_keys={"mrope_section"})
        self.image_token_id = image_token_id
        self.video_token_id = video_token_id

        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)


class Qwen2VLConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`Qwen2VLModel`]. It is used to instantiate a
    Qwen2-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of
    Qwen2-VL-7B-Instruct [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct).

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PretrainedConfig`] for more information.


    Args:
        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2_5_VLTextConfig`):
            The config object or dictionary of the text backbone.
        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2_5_VLVisionConfig`):
            The config object or dictionary of the vision backbone.
        image_token_id (`int`, *optional*, defaults to 151655):
            The image token index to encode the image prompt.
        video_token_id (`int`, *optional*, defaults to 151656):
            The video token index to encode the image prompt.

    ```python
    >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig

    >>> # Initializing a Qwen2_5_VL style configuration
    >>> configuration = Qwen2_5_VLConfig()

    >>> # Initializing a model from the Qwen2-VL-7B style configuration
    >>> model = Qwen2_5_VLForConditionalGeneration(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "qwen2_vl"
    sub_configs = {"vision_config": Qwen2VLVisionConfig, "text_config": Qwen2VLTextConfig}
    keys_to_ignore_at_inference = ["past_key_values"]

    def __init__(
        self,
        text_config=None,
        vision_config=None,
        image_token_id=151655,
        video_token_id=151656,
        **kwargs,
    ):
        if isinstance(vision_config, dict):
            self.vision_config = self.sub_configs["vision_config"](**vision_config)
        elif vision_config is None:
            self.vision_config = self.sub_configs["vision_config"]()

        if isinstance(text_config, dict):
            self.text_config = self.sub_configs["text_config"](**text_config)
        elif text_config is None:
            # For BC use all kwargs to init `TextConfig`
            self.text_config = self.sub_configs["text_config"](**kwargs)

        self.image_token_id = image_token_id
        self.video_token_id = video_token_id

        super().__init__(**kwargs)


__all__ = ["Qwen2VLConfig", "Qwen2VLTextConfig"]




# coding=utf-8
# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch Qwen2-VL model."""

from dataclasses import dataclass
from typing import Any, Callable, Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import LayerNorm

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import BaseModelOutputWithPast, ModelOutput
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import (
    TransformersKwargs,
    auto_docstring,
    can_return_tuple,
    is_torchdynamo_compiling,
    logging,
)
from transformers.utils.deprecation import deprecate_kwarg
from transformers.models.qwen2.modeling_qwen2 import (
    Qwen2RMSNorm,
)
# from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig


logger = logging.get_logger(__name__)


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Llava outputs, with hidden states and attentions.
    """
)
class Qwen2VLModelOutputWithPast(ModelOutput):
    r"""
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Qwen2VL causal language model (or autoregressive) outputs.
    """
)
class Qwen2VLCausalLMOutputWithPast(ModelOutput):
    r"""
    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
        Language modeling loss (for next-token prediction).
    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


class Qwen2VLRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen2VLTextConfig, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        # In contrast to other models, Qwen2_VL has different position ids for the grids
        # So we expand the inv_freq to shape (3, ...)
        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


# Copied from transformers.models.llama.modeling_llama.rotate_half
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):
    """Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors (https://qwenlm.github.io/blog/qwen2-vl/).

    Explanation:
        Multimodal 3D rotary position embedding is an extension to 1D rotary position embedding. The input embedding
        sequence contains vision (images / videos) embedding and text embedding or just contains text embedding. For
        vision embedding part, we apply rotary position embedding on temporal, height and width dimension separately.
        Here we split the channel dimension to 3 chunks for the temporal, height and width rotary position embedding.
        For text embedding part, we just apply 1D rotary position embedding. The three rotary position index (temporal,
        height and width) of text embedding is always the same, so the text embedding rotary position embedding has no
        difference with modern LLMs.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`):
            The position indices of the tokens corresponding to the query and key tensors. For example, this can be
            used to pass offsetted position ids when working with a KV-cache.
        mrope_section(`List(int)`):
            Multimodal rope section is for channel dimension of temporal, height and width in rope calculation.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    mrope_section = mrope_section * 2
    cos = torch.cat([m[i % 3] for i, m in enumerate(cos.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )
    sin = torch.cat([m[i % 3] for i, m in enumerate(sin.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )

    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def apply_rotary_pos_emb_vision(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()
    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)
    return q_embed, k_embed


class VisionRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, dim: int, theta: float = 10000.0) -> None:
        super().__init__()
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def forward(self, seqlen: int) -> torch.Tensor:
        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(seq, self.inv_freq)
        return freqs


class PatchEmbed(nn.Module):
    def __init__(
        self,
        patch_size: int = 14,
        temporal_patch_size: int = 2,
        in_channels: int = 3,
        embed_dim: int = 1152,
    ) -> None:
        super().__init__()
        self.patch_size = patch_size
        self.temporal_patch_size = temporal_patch_size
        self.in_channels = in_channels
        self.embed_dim = embed_dim

        kernel_size = [temporal_patch_size, patch_size, patch_size]
        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=False)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        target_dtype = self.proj.weight.dtype
        hidden_states = hidden_states.view(
            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
        )
        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
        return hidden_states


class PatchMerger(nn.Module):
    def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> None:
        super().__init__()
        self.hidden_size = context_dim * (spatial_merge_size**2)
        self.ln_q = LayerNorm(context_dim, eps=1e-6)
        self.mlp = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.GELU(),
            nn.Linear(self.hidden_size, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))
        return x


class VisionMlp(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, hidden_act: str) -> None:
        super().__init__()
        self.fc1 = nn.Linear(dim, hidden_dim)
        self.act = ACT2FN[hidden_act]
        self.fc2 = nn.Linear(hidden_dim, dim)

    def forward(self, x) -> torch.Tensor:
        return self.fc2(self.act(self.fc1(x)))


# Copied from transformers.models.llama.modeling_llama.repeat_kv
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class VisionAttention(nn.Module):
    def __init__(self, config: Qwen2VLVisionConfig) -> None:
        super().__init__()
        self.dim = config.embed_dim
        self.num_heads = config.num_heads
        self.head_dim = self.dim // self.num_heads
        self.num_key_value_groups = 1  # needed for eager attention
        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)
        self.proj = nn.Linear(self.dim, self.dim)
        self.scaling = self.head_dim**-0.5
        self.config = config
        self.attention_dropout = 0.0
        self.is_causal = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        query_states, key_states, value_states = (
            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        )
        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)

        query_states = query_states.transpose(0, 1).unsqueeze(0)
        key_states = key_states.transpose(0, 1).unsqueeze(0)
        value_states = value_states.transpose(0, 1).unsqueeze(0)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if self.config._attn_implementation == "flash_attention_2":
            # Flash Attention 2: Use cu_seqlens for variable length attention
            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
            attn_output, _ = attention_interface(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask=None,
                scaling=self.scaling,
                dropout=0.0 if not self.training else self.attention_dropout,
                cu_seq_lens_q=cu_seqlens,
                cu_seq_lens_k=cu_seqlens,
                max_length_q=max_seqlen,
                max_length_k=max_seqlen,
                is_causal=False,
                **kwargs,
            )
        else:
            # Other implementations: Process each chunk separately
            lengths = cu_seqlens[1:] - cu_seqlens[:-1]
            splits = [
                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
            ]

            attn_outputs = [
                attention_interface(
                    self,
                    q,
                    k,
                    v,
                    attention_mask=None,
                    scaling=self.scaling,
                    dropout=0.0 if not self.training else self.attention_dropout,
                    is_causal=False,
                    **kwargs,
                )[0]
                for q, k, v in zip(*splits)
            ]
            attn_output = torch.cat(attn_outputs, dim=1)

        attn_output = attn_output.reshape(seq_length, -1).contiguous()
        attn_output = self.proj(attn_output)
        return attn_output


class Qwen2VLVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config, attn_implementation: str = "sdpa") -> None:
        super().__init__()
        self.norm1 = LayerNorm(config.embed_dim, eps=1e-6)
        self.norm2 = LayerNorm(config.embed_dim, eps=1e-6)
        mlp_hidden_dim = int(config.embed_dim * config.mlp_ratio)

        self.attn = VisionAttention(config=config)
        self.mlp = VisionMlp(dim=config.embed_dim, hidden_dim=mlp_hidden_dim, hidden_act=config.hidden_act)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        hidden_states = hidden_states + self.attn(
            self.norm1(hidden_states),
            cu_seqlens=cu_seqlens,
            rotary_pos_emb=rotary_pos_emb,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
        return hidden_states


# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2MLP
class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class Qwen2VLAttention(nn.Module):
    """
    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer
    and "Generating Long Sequences with Sparse Transformers".
    """

    def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "
                "to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.is_causal = True
        self.attention_dropout = config.attention_dropout
        self.rope_scaling = config.rope_scaling
        self.scaling = self.head_dim**-0.5

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

        self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_values is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            position_ids=position_ids,  # pass positions for FA2
            **kwargs,
        )

        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen2VLDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        if config.use_sliding_window and config._attn_implementation != "flash_attention_2":
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )
        self.self_attn = Qwen2VLAttention(config, layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, sequence_length)` where padding elements are indicated by 0.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_values (`Cache`, *optional*): cached past key and value projection states
            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
                Indices depicting the position of the input sequence tokens in the sequence.
            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):
                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,
                with `head_dim` being the embedding dimension of each attention head.
            kwargs (`dict`, *optional*):
                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code
                into the model
        """

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        return outputs


@auto_docstring
class Qwen2VLPreTrainedModel(PreTrainedModel):
    config: Qwen2VLConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen2VLDecoderLayer", "Qwen2VLVisionBlock"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True


@auto_docstring
class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):
    config: Qwen2VLVisionConfig
    _no_split_modules = ["Qwen2VLVisionBlock"]

    def __init__(self, config) -> None:
        super().__init__(config)
        self.spatial_merge_size = config.spatial_merge_size

        self.patch_embed = PatchEmbed(
            patch_size=config.patch_size,
            temporal_patch_size=config.temporal_patch_size,
            in_channels=config.in_channels,
            embed_dim=config.embed_dim,
        )

        head_dim = config.embed_dim // config.num_heads
        self.rotary_pos_emb = VisionRotaryEmbedding(head_dim // 2)

        self.blocks = nn.ModuleList([Qwen2VLVisionBlock(config) for _ in range(config.depth)])
        self.merger = PatchMerger(
            dim=config.hidden_size, context_dim=config.embed_dim, spatial_merge_size=config.spatial_merge_size
        )
        self.gradient_checkpointing = False

    def get_dtype(self) -> torch.dtype:
        return self.blocks[0].mlp.fc2.weight.dtype

    def get_device(self) -> torch.device:
        return self.blocks[0].mlp.fc2.weight.device

    def rot_pos_emb(self, grid_thw):
        pos_ids = []
        for t, h, w in grid_thw:
            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
            hpos_ids = hpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            hpos_ids = hpos_ids.permute(0, 2, 1, 3)
            hpos_ids = hpos_ids.flatten()

            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
            wpos_ids = wpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            wpos_ids = wpos_ids.permute(0, 2, 1, 3)
            wpos_ids = wpos_ids.flatten()
            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
        pos_ids = torch.cat(pos_ids, dim=0)
        max_grid_size = grid_thw[:, 1:].max()
        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
        return rotary_pos_emb

    @auto_docstring
    def forward(
        self,
        hidden_states: torch.Tensor,
        grid_thw: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        r"""
        grid_thw (`torch.LongTensor` of shape `(num_images, 3)`):
            The temporal, height and width dimensions of feature shape for each image. Each row contains [t, h, w] values.
        """
        # if len(hidden_states.shape) == 4:  # Image Input, (B, C, H, W)
        #     t = 1
        #     h = hidden_states.size(2) // 14
        #     w = hidden_states.size(3) // 14
        # else:  # Video Input, (B, C, T, H, W)
        #     t = hidden_states.size(2)
        #     h = hidden_states.size(3) // 14
        #     w = hidden_states.size(4) // 14

        # # æž„é€  (B, 3) çš„ grid_thw
        # grid_thw = hidden_states.new_tensor([t, h, w], dtype=torch.long).unsqueeze(0).repeat(hidden_states.size(0), 1)

        hidden_states = self.patch_embed(hidden_states)
        rotary_pos_emb = self.rot_pos_emb(grid_thw)
        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
        position_embeddings = (emb.cos(), emb.sin())

        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
            dim=0,
            # Select dtype based on the following factors:
            #  - FA2 requires that cu_seqlens_q must have dtype int32
            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw
            # See https://github.com/huggingface/transformers/pull/34852 for more information
            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)

        for blk in self.blocks:
            hidden_states = blk(
                hidden_states,
                cu_seqlens=cu_seqlens,
                position_embeddings=position_embeddings,
                **kwargs,
            )

        return hidden_states



















import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import torchvision.transforms as transforms
import torch.nn.functional as F



def compute_patch_similarity_heatmap(patch_features, H, W, target_patch_coord):
    """
    è®¡ç®—æŒ‡å®špatchä¸Žå…¶ä»–æ‰€æœ‰patchçš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œå¹¶ç”Ÿæˆçƒ­åŠ›å›¾
    
    Args:
        patch_features: patchç‰¹å¾å¼ é‡, shape (1, num_patches, feature_dim)
        H: patchç½‘æ ¼é«˜åº¦
        W: patchç½‘æ ¼å®½åº¦  
        target_patch_coord: ç›®æ ‡patchåæ ‡ (h_idx, w_idx)
    
    Returns:
        heatmap: ç›¸ä¼¼æ€§çƒ­åŠ›å›¾, shape (H, W)
    """
    # print(patch_features.shape[1])
    assert patch_features.shape[1] == H * W, f"ç‰¹å¾æ•°é‡{H*W}ä¸Žç½‘æ ¼å¤§å°{H}x{W}ä¸åŒ¹é…"
    
    # æå–ç›®æ ‡patchçš„ç‰¹å¾
    target_idx = target_patch_coord[0] * W + target_patch_coord[1]
    target_feature = patch_features[0, target_idx]  # shape (feature_dim,)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§
    similarities = F.cosine_similarity(
        target_feature.unsqueeze(0),  # shape (1, feature_dim)
        patch_features[0],            # shape (num_patches, feature_dim)
        dim=1
    )
    
    # é‡å¡‘ä¸º2Dçƒ­åŠ›å›¾
    heatmap = similarities.reshape(H, W).to(torch.float32).cpu().numpy()
    return heatmap

def plot_similarity_heatmap(heatmap, target_patch_coord, save_path=None):
    """
    ç»˜åˆ¶ç›¸ä¼¼æ€§çƒ­åŠ›å›¾ï¼Œå¹¶åœ¨ç›®æ ‡patchä½ç½®æ˜¾ç¤ºçº¢ç‚¹
    
    Args:
        heatmap: ç›¸ä¼¼æ€§çƒ­åŠ›å›¾, shape (H, W)
        target_patch_coord: ç›®æ ‡patchåæ ‡ (h_idx, w_idx)
        original_img_size: åŽŸå§‹å›¾åƒå°ºå¯¸ (å¯é€‰ï¼Œç”¨äºŽè°ƒæ•´æ˜¾ç¤ºæ¯”ä¾‹)
        patch_size: æ¯ä¸ªpatchçš„åƒç´ å¤§å°
    """
    H, W = heatmap.shape
    
    fig, ax = plt.subplots(1, 1, figsize=(5, 4))
    
    # æ˜¾ç¤ºçƒ­åŠ›å›¾
    im = ax.imshow(heatmap, cmap='viridis', aspect='equal')
    
    # åœ¨ç›®æ ‡patchä½ç½®æ·»åŠ çº¢ç‚¹
    target_h, target_w = target_patch_coord
    ax.plot(target_w, target_h, 'ro', markersize=10, markeredgecolor='white', markeredgewidth=2)
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im, ax=ax, label='Cosine Similarity')
    
    # è®¾ç½®åæ ‡è½´æ ‡ç­¾
    ax.set_xlabel('Width (patch index)')
    ax.set_ylabel('Height (patch index)')
    ax.set_title(f'Cosine Similarity to Patch at ({target_h}, {target_w})')
    
    # è®¾ç½®ç½‘æ ¼çº¿
    ax.set_xticks(np.arange(-0.5, W, 1), minor=True)
    ax.set_yticks(np.arange(-0.5, H, 1), minor=True)
    ax.grid(which="minor", color="white", linestyle='-', linewidth=0.5)
    ax.tick_params(which="minor", size=0)
    
    # è®¾ç½®ä¸»åˆ»åº¦
    ax.set_xticks(np.arange(0, W, max(1, W//10)))
    ax.set_yticks(np.arange(0, H, max(1, H//10)))
    
    plt.tight_layout()

    if save_path is not None:
        fig.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Heatmap saved to {save_path}")

    plt.show()
    
    return fig, ax


















import os
import json
from safetensors.torch import load_file

# model_path = "/video_vit/huggingface/pretrian_LLM/Qwen2.5-VL-3B-Instruct"

def RiceEncoder():
    # 1. åˆå§‹åŒ–
    model_dir = "/video_vit/huggingface/pretrian_LLM/Qwen2-VL-7B-Instruct"
    config = Qwen2VLVisionConfig.from_pretrained(model_dir)
    # config = Qwen2VLVisionConfig.from_pretrained(model_path)
    print(config)
    # print("è¿™é‡Œ")
    encoder = Qwen2VisionTransformerPretrainedModel._from_config(config)

    
    # 1. è¯»å– index.json
    index_file = os.path.join(model_dir, "model.safetensors.index.json")
    with open(index_file, "r") as f:
        index = json.load(f)

    # 2. æ‰¾åˆ°æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶å
    weight_map = index["weight_map"]  # dict: {param_name: filename}
    shards = sorted(set(weight_map.values()))  # åŽ»é‡+æŽ’åº
    print("æ‰¾åˆ°çš„åˆ†ç‰‡æ–‡ä»¶ï¼š", shards)

    # 3. ä¾æ¬¡åŠ è½½åˆ†ç‰‡
    state_dict = {}
    for shard in shards:
        shard_path = os.path.join(model_dir, shard)
        state_dict.update(load_file(shard_path))

    # 3. åªä¿ç•™ vision æ¨¡å—çš„å‚æ•°
    vision_state_dict = {k.replace("visual.", ""): v for k, v in state_dict.items() if k.startswith("visual.")}

    # print("ç¤ºä¾‹åŽŸå§‹ key:", list(state_dict.keys())[-20:])
    # print("ç¤ºä¾‹ vision key:", list(vision_state_dict.keys())[:20])
    # print("æ¨¡åž‹å‚æ•°å:", list(encoder.state_dict().keys())[:20])

    # 4. åŠ è½½æƒé‡
    encoder.load_state_dict(vision_state_dict, strict=True)

    return encoder

model = RiceEncoder()




from transformers import Qwen2_5_VLModel, AutoProcessor

model_path = "/video_vit/huggingface/pretrian_LLM/Qwen2-VL-7B-Instruct"

# model = Qwen2_5_VLModel.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto")
processor = AutoProcessor.from_pretrained(model_path)

from qwen_vl_utils import process_vision_info
url = "/video_vit/utils_haolin/deduplication/cifar10_images/dog.jpg"
prompt = "1"
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "images",
                "image": url,
            },
            {
                "type": "text",
                "text": prompt,
            }
        ]
    }
]
text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
image_inputs, video_inputs, _ = process_vision_info([messages], return_video_kwargs=True)

inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)

inputs = inputs.to(torch.device(model.device))



# print(inputs)
# with torch.no_grad():
img_feature = model.forward(inputs['pixel_values'], inputs['image_grid_thw'])
print(img_feature.shape)

qwen_feature = img_feature.reshape(1, -1, 1280)

# from transformers import Qwen2VLForConditionalGeneration, AutoProcessor

# model_path = "/video_vit/huggingface/pretrian_LLM/Qwen2-VL-7B-Instruct"

# model_hf = model = Qwen2VLForConditionalGeneration.from_pretrained(
#     "/video_vit/huggingface/pretrian_LLM/Qwen2-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
# )
# processor_hf = AutoProcessor.from_pretrained(model_path)

# print(inputs['image_grid_thw'])
# vit = model_hf.visual.eval()

# img_feature_hf = vit.forward(inputs['pixel_values'], inputs['image_grid_thw'])


# qwen_feature_hf = img_feature_hf.reshape(1, -1, 3584)

# img_size=224

with torch.inference_mode():
    # patch_size = 14
    # H = W = img_size // patch_size
    # print(f"Patch grid: H={H}, W={W}")
    # ä½ é€‰ä¸­çš„ç›®æ ‡patchçš„åæ ‡ï¼Œæ³¨æ„è¦åœ¨[0,H-1]å’Œ[0,W-1]èŒƒå›´å†…ã€‚
    target_patch_coord = (20, 20)
    # print(outputs.shape)
    heatmap_qwen = compute_patch_similarity_heatmap(qwen_feature, 86, 114, target_patch_coord)

    plot_similarity_heatmap(heatmap_qwen, target_patch_coord, "/video_vit/llava-onevision/Llava-vit-v0/model_factory/qwen_vit_without_merger.png")
    # heatmap_qwen_hf = compute_patch_similarity_heatmap(qwen_feature_hf, 43, 57, target_patch_coord)

    # plot_similarity_heatmap(heatmap_qwen_hf, target_patch_coord, "/video_vit/llava-onevision/Llava-vit-v0/model_factory/qwen_vit_hf.png")